{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac037a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle \n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training.example import Example\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713743f",
   "metadata": {},
   "source": [
    "### Train NER model on Data of 200 Resume\n",
    "Data Source: https://github.com/laxmimerit/Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3c21c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e061fc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = pickle.load(open(\"../data/train_data.pkl\",\"rb\"))\n",
    "type(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a09c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a079ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\") # load a new spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9486cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "?doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "040d1599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fedd8260eb4948a15098211ac39be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 226 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kb/v0p0ypbd0wx1q2qb1kpxmmcc0000gn/T/ipykernel_57170/1662460391.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0ments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ments\u001b[0m \u001b[0;31m# label the text with the ents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 226 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "db = DocBin() # create a DocBin object\n",
    "\n",
    "for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    doc = nlp.make_doc(text) # create doc object from text\n",
    "    ents = []\n",
    "    for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    doc.ents = ents # label the text with the ents\n",
    "    db.add(doc)\n",
    "\n",
    "db.to_disk(\"../data/train.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e177",
   "metadata": {},
   "source": [
    "### Initialise blank ner model, and add entities from training data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42655c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = spacy.blank(\"en\")\n",
    "ner_pipe = ner_model.add_pipe(\"ner\",last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eea3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,annotation in tran_data_org:\n",
    "    for entity in annotation[\"entities\"]:\n",
    "        ner_pipe.add_label(entity[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a944f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = tran_data_org.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c253975",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b6d8ed7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(2128, 2144, 'Companies worked at')' and '(2116, 2143, 'College Name')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kb/v0p0ypbd0wx1q2qb1kpxmmcc0000gn/T/ipykernel_57170/2992728724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/training/example.pyx\u001b[0m in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/resume_parser/lib/python3.9/site-packages/spacy/training/iob_utils.py\u001b[0m in \u001b[0;36moffsets_to_biluo_tags\u001b[0;34m(doc, entities, missing)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_in_ents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    107\u001b[0m                         Errors.E103.format(\n\u001b[1;32m    108\u001b[0m                             span1=(\n",
      "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(2128, 2144, 'Companies worked at')' and '(2116, 2143, 'College Name')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead."
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    for text, annotations in batch:\n",
    "        doc = ner_model.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10e7527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entities': [(1894, 2173, 'Skills'),\n",
       "   (1726, 1851, 'Skills'),\n",
       "   (1711, 1716, 'Graduation Year'),\n",
       "   (1643, 1678, 'College Name'),\n",
       "   (1610, 1642, 'Degree'),\n",
       "   (1433, 1468, 'College Name'),\n",
       "   (1385, 1390, 'Graduation Year'),\n",
       "   (1359, 1363, 'Location'),\n",
       "   (1327, 1356, 'College Name'),\n",
       "   (1276, 1325, 'Degree'),\n",
       "   (269, 274, 'Graduation Year'),\n",
       "   (242, 246, 'Location'),\n",
       "   (204, 238, 'Companies worked at'),\n",
       "   (181, 202, 'Designation'),\n",
       "   (145, 149, 'Location'),\n",
       "   (33, 37, 'Location'),\n",
       "   (17, 31, 'Designation'),\n",
       "   (0, 16, 'Name')]},\n",
       " {'entities': [(1520, 1524, 'Graduation Year'),\n",
       "   (1507, 1518, 'Degree'),\n",
       "   (1496, 1500, 'Graduation Year'),\n",
       "   (1488, 1492, 'Graduation Year'),\n",
       "   (1440, 1463, 'College Name'),\n",
       "   (1415, 1439, 'Degree'),\n",
       "   (1409, 1413, 'Graduation Year'),\n",
       "   (1334, 1366, 'College Name'),\n",
       "   (1291, 1333, 'Degree'),\n",
       "   (917, 940, 'Skills'),\n",
       "   (413, 417, 'Graduation Year'),\n",
       "   (405, 409, 'Graduation Year'),\n",
       "   (356, 368, 'Skills'),\n",
       "   (313, 318, 'Companies worked at'),\n",
       "   (51, 101, 'Email Address'),\n",
       "   (12, 23, 'Location'),\n",
       "   (0, 11, 'Name')]},\n",
       " {'entities': [(5510, 5514, 'Graduation Year'),\n",
       "   (5481, 5508, 'College Name'),\n",
       "   (5475, 5479, 'Graduation Year'),\n",
       "   (5426, 5440, 'College Name'),\n",
       "   (5419, 5425, 'Degree'),\n",
       "   (4077, 4085, 'Companies worked at'),\n",
       "   (1810, 1818, 'Companies worked at'),\n",
       "   (1789, 1797, 'Companies worked at'),\n",
       "   (1760, 1768, 'Companies worked at'),\n",
       "   (1725, 1758, 'Designation'),\n",
       "   (1576, 1706, 'Skills'),\n",
       "   (1555, 1563, 'Companies worked at'),\n",
       "   (155, 166, 'Years of Experience'),\n",
       "   (101, 145, 'Email Address'),\n",
       "   (60, 67, 'Location'),\n",
       "   (50, 58, 'Companies worked at'),\n",
       "   (14, 47, 'Designation'),\n",
       "   (0, 13, 'Name')]},\n",
       " {'entities': [(3052, 3067, 'Skills'),\n",
       "   (2993, 3016, 'College Name'),\n",
       "   (2951, 2992, 'Degree'),\n",
       "   (870, 878, 'Companies worked at'),\n",
       "   (847, 868, 'Designation'),\n",
       "   (600, 608, 'Companies worked at'),\n",
       "   (577, 598, 'Designation'),\n",
       "   (116, 167, 'Email Address'),\n",
       "   (73, 82, 'Location'),\n",
       "   (19, 40, 'Designation'),\n",
       "   (0, 19, 'Name')]})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c57556c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(model,TRAIN_DATA,n_iter=20):\n",
    "    model.begin_training()\n",
    "    for itn in tqdm(range(n_iter)):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            doc = model.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            model.update(\n",
    "                example,\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        print(f\"For itr {itn}, Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "185763ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "?spacy.training.example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8c55a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a369b763ffd649ceb7a51eedf751dd61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'example_dict' has incorrect type (expected dict, got tuple)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kb/v0p0ypbd0wx1q2qb1kpxmmcc0000gn/T/ipykernel_57170/3488828400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtran_data_org\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/kb/v0p0ypbd0wx1q2qb1kpxmmcc0000gn/T/ipykernel_57170/2190957583.py\u001b[0m in \u001b[0;36mtrain_ner\u001b[0;34m(model, TRAIN_DATA, n_iter)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             model.update(\n\u001b[1;32m     13\u001b[0m                 \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'example_dict' has incorrect type (expected dict, got tuple)"
     ]
    }
   ],
   "source": [
    "train_ner(ner_model,tran_data_org.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6762d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "?ner_model.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "870dfb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        # reset and initialize the weights randomly â€“ but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcc211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc628b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7096c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f684ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5dff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_streamlit\n",
    "import typer\n",
    "\n",
    "\n",
    "def main(models: str, default_text: str):\n",
    "    models = [name.strip() for name in models.split(\",\")]\n",
    "    spacy_streamlit.visualize(models, default_text, visualizers=[\"ner\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
